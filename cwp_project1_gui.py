# -*- coding: utf-8 -*-
"""cwp_project1_gui.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkOag2ypY8fm2dFDPIQDOyw5vm8VjdQ0
"""
#imprting necessary packages
import tkinter
from tkinter import *
import tkinter
import cv2
from PIL import Image, ImageTk
import os
import numpy as np
import cv2
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout, Flatten
from tensorflow.python.keras.layers import Conv2D
from tensorflow.keras.optimizers import Adam
from tensorflow.python.keras.layers import MaxPooling2D
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
import threading
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'

#making model using squential and adding layers
emotion_model = Sequential()
emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))
emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))

emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))

emotion_model.add(Flatten())
emotion_model.add(Dense(1024, activation='relu'))
emotion_model.add(Dropout(0.5))
emotion_model.add(Dense(7, activation='softmax'))
emotion_model.save_weights('model4.h5')

cv2.ocl.setUseOpenCL(False)

emotion_dict = {0: "   Angry   ",1: "    Disgusted   " ,2:"   Frearful   ",3:"   Happy   ",4:"   Neutral   ",5:"   Sad   ",6 : "   Surprised   " }

cur_path = os.path.dirname(os.path.abspath(__file__))

emoji_dist = {0:cur_path+"/emojis/angry.png",1:cur_path+"/emojis/disgusted.png",2:cur_path+"/emojis/fearful.png",3:cur_path+"/emojis/happy.png",4:cur_path+"/emojis/neutral.png",5:cur_path+"/emojis/sad.png",6:cur_path+"/emojis/surpriced.png",}

global last_frame1

last_frame1 = np.zeros((480 , 640, 3), dtype=np.uint8)
global cap1
show_text=[0]
global frame_number

#video processing function
def show_subject():      
    cap1 = cv2.VideoCapture(r'D:\Visual Studio\cwp_project\WIN_20210712_16_26_48_Pro.mp4')    #to open our video/webcam                             
    if not cap1.isOpened():                             
        print("cant open the camera1")
    global frame_number
    length = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_number += 1 
    if frame_number >= length:
        exit()
    cap1.set(1,frame_number)
    flag1, frame1 = cap1.read()     #flag1 tells us if we reading soemthing or not  and frame1 is an array btowh would be returned by read() function 
    frame1 = cv2.resize (frame1,(600,500)) #resizing the camera
    bounding_box = cv2.CascadeClassifier('C:/Users/rajat/AppData/Local/Programs/Python/Python39/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')
    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor =1.3 ,minNeighbors = 5)
    for(x,y,w,h) in num_faces:
        cv2.rectangle(frame1,(x,y-50),(x+w , y+h+10),(255,0,0) ,2)
        roi_gray_frame = gray_frame[y:y + h, x:x + w]
        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)), -1 ), 0)
        prediction = emotion_model.predict(cropped_img)
        maxindex= int(np.argmax(prediction))
        cv2.putText(frame1, emotion_dict[maxindex], (x+20 , y-60), cv2.FONT_HERSHEY_SIMPLEX ,1, (255, 255, 255), 2, cv2.LINE_AA)
        show_text[0] = maxindex
    if flag1 is None:
        print ("Major error!")
    elif flag1:
        global last_frame1
        last_frame1= frame1.copy()
        pic = cv2.cvtColor(last_frame1 ,cv2.COLOR_BGR2RGB)
        img = Image.fromarray(pic)
        imgtk = ImageTk.PhotoImage(image=img)
        lmain.imgtk = imgtk
        lmain.configure(image=imgtk)
        root.update()
        lmain.after(10, show_subject)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        exit()

def show_avatar():   #avatar/ emoji placement
    frame2=cv2.imread(emoji_dist[show_text[0]])
    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)
    img2=Image.fromarray(frame2)
    imgtk2=ImageTk.PhotoImage(image=img2)
    lmain2.imgtk2=imgtk2
    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))
    
    lmain2.configure(image=imgtk2)
    root.update()
    lmain2.after(10, show_avatar)

if __name__ == '__main__':
    frame_number = 0          #global variable
    root=tkinter.Tk()
    lmain = tkinter.Label(master=root,padx=50,bd=10)    #labels which contents images/video
    lmain2 = tkinter.Label(master=root,bd=10)           #labels which contents images/video
    lmain3 = tkinter.Label(master=root,bd=10, fg="#CDCDCD", bg='grey')
    lmain.pack(side=LEFT)
    lmain.place(x=50,y=250)
    lmain3.pack()
    lmain3.place(x=960,y=250)
    lmain2.pack(side=RIGHT)
    lmain2.place(x=900,y=350)

    root.title("Photo of Emoji")   #specified title
    root.geometry("1400x900+100+10")
    root['bg']='black'    #gui background
    exitButton = Button(root, text='Quit',fg = "red", command=root.destroy, font= ('arial',25,'bold')).pack (side= BOTTOM)
    threading.Thread(target=show_subject).start()   #subject function  #in new threads
    threading.Thread(target=show_avatar).start()    #avatar function 
    root.mainloop()   